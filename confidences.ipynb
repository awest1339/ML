{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision function shape: (38, 3)\nDecision function:\n[[-0.52931069  1.46560359 -0.50448467]\n [ 1.51154215 -0.49561142 -0.50310736]\n [-0.52379401 -0.4676268   1.51953786]\n [-0.52931069  1.46560359 -0.50448467]\n [-0.53107259  1.28190451  0.21510024]\n [ 1.51154215 -0.49561142 -0.50310736]]\nArgmax of decision function: \n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n 0]\nPredictions:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n 0]\nPred = gbrt: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, make_moons, make_blobs, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import pandas as pd\n",
    "\n",
    "import mglearn\n",
    "\n",
    "\n",
    "def plot_feature_importances(model, dataset):\n",
    "    n_features = dataset.data.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), dataset.feature_names)\n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "\n",
    "def run_random_forest():\n",
    "    cancer = load_breast_cancer()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        cancer.data, cancer.target, random_state=0)\n",
    "    forest = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "    forest.fit(X_train, y_train)\n",
    "    print('Performance of train data: {:.3f}'.format(forest.score(X_train, y_train)))\n",
    "    print('Performance of test data: {:.3f}'.format(forest.score(X_test, y_test)))\n",
    "    plot_feature_importances(forest, cancer)\n",
    "    \n",
    "\n",
    "def run_gradient_boosting_classifier():\n",
    "    '''\n",
    "    Gradient booster is an ensemble learning, combining weak learners to improve\n",
    "    results. \n",
    "    :return: Returns None.\n",
    "    '''\n",
    "    X, y = make_circles(noise=.25, factor=.5, random_state=1)\n",
    "    # rename the classes blue and red for visualization purposes\n",
    "    y_named = np.array(['blue', 'red'])[y]\n",
    "    X_train, X_test, y_train_named, y_test_named, y_train, y_test = train_test_split(\n",
    "        X, y_named, y, random_state=0\n",
    "    )\n",
    "    # Build the gradient boosting model and fit it to the data\n",
    "    gbrt = GradientBoostingClassifier(random_state=0)\n",
    "    gbrt.fit(X_train, y_train_named)\n",
    "\n",
    "    greater_zero = (gbrt.decision_function(X_test) > 0).astype(int)\n",
    "    pred = gbrt.classes_[greater_zero]\n",
    "    # print('X_test.shape: {}'.format(X_test.shape))\n",
    "    # print('Decision function shape: {}'.format(gbrt.decision_function(X_test).shape))\n",
    "    # print('Decision function: \\n{}'.format(gbrt.decision_function(X_test)[:6]))\n",
    "    # print('Thresholded decision function:\\n{}'.format(gbrt.decision_function(X_test) > 0))\n",
    "    # print('Predictions:\\n{}'.format(gbrt.predict(X_test)))\n",
    "    # print('pred is equal to predictions: {}'.format(\n",
    "    #  np.all(pred==gbrt.predict(X_test))))\n",
    "    # decision_function = gbrt.decision_function(X_test)\n",
    "    # print('Decision function min: {:.2f}'.format(np.min(decision_function)))\n",
    "    # print('Decision function max: {:.2f}'.format(np.max(decision_function)))\n",
    "    \n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "    # mglearn.tools.plot_2d_separator(gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)\n",
    "    # scores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[1], alpha=.4, cm=mglearn.ReBl)\n",
    "    # for ax in axes:\n",
    "    #     mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test, markers='^', ax=ax)\n",
    "    #     mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, markers='o', ax=ax)\n",
    "    #     ax.set_xlabel('Feature 0')\n",
    "    #     ax.set_ylabel('Feature 1')\n",
    "    #     cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
    "    #     axes[0].legend(['Test class 0', 'Test class 1', 'Train class 0', 'Train class 1'], ncol=4, loc=(.1, 1.1))\n",
    "    print('Shape of probabilities: {}'.format(gbrt.predict_proba(X_test).shape))\n",
    "    # print('Probabilities:') \n",
    "    # for item in gbrt.predict_proba(X_test):\n",
    "    #     print('{}, {}'.format(item[0], item[1]))\n",
    "    \n",
    "    \n",
    "def run_multiclass_GBC():\n",
    "    iris = load_iris()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
    "    gbrt = GradientBoostingClassifier(learning_rate=.01, random_state=0)\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    print('Decision function shape: {}'.format(gbrt.decision_function(X_test).shape))\n",
    "    print('Decision function:\\n{}'.format(gbrt.decision_function(X_test)[:6, :]))\n",
    "    print('Argmax of decision function: \\n{}'.format(np.argmax(gbrt.decision_function(X_test), axis=1)))\n",
    "    print('Predictions:\\n{}'.format(gbrt.predict(X_test)))\n",
    "    print('Pred = gbrt: {}'.format(\n",
    "        np.all(gbrt.predict(X_test)==np.argmax(gbrt.decision_function(X_test), axis=1))))\n",
    "    \n",
    "    # print('Predicted probs: {}'.format(gbrt.predict_proba(X_test)))\n",
    "    # print('Sums: {}'.format(gbrt.predict_proba(X_test)[:6].sum(axis=1)))\n",
    "    # print('Argmax of predicted proba: {}'.format(np.argmax(gbrt.predict_proba(X_test), axis=1)))\n",
    "    # print('Predictions:\\n{}'.format(gbrt.predict(X_test)))\n",
    "          \n",
    "def run_linear_SVC():\n",
    "    cancer = load_breast_cancer()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        cancer.data, cancer.target, random_state=0)\n",
    "    min_on_training = X_train.min(axis=0)\n",
    "    range_on_training = (X_train - min_on_training).max(axis=0)\n",
    "    X_train_scaled = (X_train - min_on_training) / range_on_training\n",
    "    X_test_scaled = (X_test - min_on_training) / range_on_training\n",
    "    \n",
    "    svc = SVC(C=1000).fit(X_train_scaled, y_train)\n",
    "    print('Performance on train data: {:.3f}'.format(svc.score(X_train_scaled, y_train)))\n",
    "    print('Performance on test data: {:.3f}\\n'.format(svc.score(X_test_scaled, y_test)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    run_multiclass_GBC()\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}